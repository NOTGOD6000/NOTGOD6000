{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ7EN4TXnKCmObN59wwJA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NOTGOD6000/NOTGOD6000/blob/main/Untitled19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "train=pd.read_csv(\"train.csv\")\n",
        "test=pd.read_csv(\"test.csv\")\n",
        "# Load data\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature Engineering\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "# Train-validation split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBM datasets\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
        "\n",
        "# Parameters\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Training\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    valid_sets=[train_data, valid_data],\n",
        "    valid_names=['train', 'valid'],\n",
        "    num_boost_round=1000,\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=50)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Prediction and Evaluation\n",
        "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "print(\"LightGBM Validation R2 with feature engineering:\", r2_score(y_valid, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m29pAPURg4E",
        "outputId": "01d9d813-04c0-469d-b50a-c386c57598f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttrain's rmse: 4.64226\tvalid's rmse: 5.15941\n",
            "[100]\ttrain's rmse: 3.93122\tvalid's rmse: 4.96798\n",
            "[150]\ttrain's rmse: 3.43076\tvalid's rmse: 4.90059\n",
            "[200]\ttrain's rmse: 3.0314\tvalid's rmse: 4.8734\n",
            "[250]\ttrain's rmse: 2.69365\tvalid's rmse: 4.84962\n",
            "Early stopping, best iteration is:\n",
            "[239]\ttrain's rmse: 2.76226\tvalid's rmse: 4.84616\n",
            "LightGBM Validation R2 with feature engineering: 0.44016539694931933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load your train.csv\n",
        "train = pd.read_csv('train.csv')\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature Engineering\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "# Log-transform the target\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# Split\n",
        "X_train, X_valid, y_train_log, y_valid = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBM datasets\n",
        "train_data = lgb.Dataset(X_train, label=y_train_log)\n",
        "valid_data = lgb.Dataset(X_valid, label=np.log1p(y_valid))\n",
        "\n",
        "# Parameters\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Train\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    valid_sets=[train_data, valid_data],\n",
        "    valid_names=['train', 'valid'],\n",
        "    num_boost_round=1000,\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=50)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Predict and inverse log\n",
        "y_pred_log = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "\n",
        "# R2 Score\n",
        "print(\"Log-transformed LightGBM R2:\", r2_score(np.expm1(y_valid), y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTB9IRyrV0wV",
        "outputId": "73d5b198-cdc4-432c-a751-70d6d8fc3be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttrain's rmse: 0.0307086\tvalid's rmse: 3.22594\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's rmse: 0.0438801\tvalid's rmse: 3.22535\n",
            "Log-transformed LightGBM R2: 0.020977588270903236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RdFokaaQPTl",
        "outputId": "0b322653-68e6-4b44-b0e2-f6cc78dace47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
            "Fast Tuned LightGBM R2: 0.4193542365673183\n",
            "Best Params: {'subsample': 0.8, 'num_leaves': 20, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature engineering (same as before)\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "# Train/validation split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fast param grid\n",
        "param_dist = {\n",
        "    'num_leaves': [20, 40],\n",
        "    'max_depth': [5, 10],\n",
        "    'learning_rate': [0.03, 0.05],\n",
        "    'n_estimators': [100, 200],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [0.8]\n",
        "}\n",
        "\n",
        "# LightGBM Regressor\n",
        "lgb_model = LGBMRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Faster RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=lgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=6,           # small number of combinations\n",
        "    cv=2,               # faster cross-validation\n",
        "    scoring='r2',\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_valid)\n",
        "\n",
        "print(\"Fast Tuned LightGBM R2:\", r2_score(y_valid, y_pred))\n",
        "print(\"Best Params:\", random_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxApT6OaVHpB",
        "outputId": "d1414610-5acc-4f38-83d9-377b8651cf11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature engineering\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. LightGBM with best params\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    subsample=0.8,\n",
        "    num_leaves=20,\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.05,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_pred = lgb_model.predict(X_valid)\n",
        "\n",
        "# 2. CatBoost\n",
        "cat_model = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    loss_function='RMSE',\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
        "cat_pred = cat_model.predict(X_valid)\n",
        "\n",
        "# 3. Random Forest\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_valid)\n",
        "\n",
        "# Blend predictions (simple average)\n",
        "final_pred = (lgb_pred + cat_pred + rf_pred) / 3\n",
        "\n",
        "# Evaluate\n",
        "print(\"Blended R2 score:\", r2_score(y_valid, final_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTpEV61-UOAb",
        "outputId": "3aba3011-790b-4c85-c909-d61f2ac25f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended R2 score: 0.4322006457962715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A: LGBM heavy\n",
        "final_pred_A = 0.7 * lgb_pred + 0.2 * cat_pred + 0.1 * rf_pred\n",
        "print(\"A: 70% LGBM, 20% Cat, 10% RF R2:\", r2_score(y_valid, final_pred_A))\n",
        "\n",
        "# B: LGBM + Cat only\n",
        "final_pred_B = 0.6 * lgb_pred + 0.4 * cat_pred\n",
        "print(\"B: 60% LGBM, 40% Cat R2:\", r2_score(y_valid, final_pred_B))\n",
        "\n",
        "# C: Equal strong models\n",
        "final_pred_C = 0.5 * lgb_pred + 0.5 * cat_pred\n",
        "print(\"C: 50% LGBM, 50% Cat R2:\", r2_score(y_valid, final_pred_C))\n",
        "\n",
        "# D: CatBoost heavy\n",
        "final_pred_D = 0.4 * lgb_pred + 0.5 * cat_pred + 0.1 * rf_pred\n",
        "print(\"D: 40% LGBM, 50% Cat, 10% RF R2:\", r2_score(y_valid, final_pred_D))\n",
        "\n",
        "# E: LGBM only (best single model)\n",
        "print(\"E: 100% LGBM R2:\", r2_score(y_valid, lgb_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZGRNboLWtru",
        "outputId": "c95f5e5a-d242-4613-f8c3-2222cdcb0066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: 70% LGBM, 20% Cat, 10% RF R2: 0.4318064648933103\n",
            "B: 60% LGBM, 40% Cat R2: 0.44334896689371717\n",
            "C: 50% LGBM, 50% Cat R2: 0.447471750530551\n",
            "D: 40% LGBM, 50% Cat, 10% RF R2: 0.44632283899384273\n",
            "E: 100% LGBM R2: 0.4193542365673183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test set\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "X_test = test.drop(columns=['id']).copy()\n",
        "\n",
        "# Apply same feature engineering\n",
        "X_test['row_mean'] = X_test.mean(axis=1)\n",
        "X_test['row_std'] = X_test.std(axis=1)\n",
        "X_test['row_max'] = X_test.max(axis=1)\n",
        "X_test['row_min'] = X_test.min(axis=1)\n",
        "X_test['row_range'] = X_test['row_max'] - X_test['row_min']\n",
        "\n",
        "# Predict using LightGBM and CatBoost only\n",
        "lgb_test_pred = lgb_model.predict(X_test)\n",
        "cat_test_pred = cat_model.predict(X_test)\n",
        "\n",
        "# Best blend\n",
        "final_test_pred = 0.5 * lgb_test_pred + 0.5 * cat_test_pred\n",
        "\n",
        "# Save to CSV\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'target': final_test_pred\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"✅ submission.csv created with R² optimized blend.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOnoxgVmXZcY",
        "outputId": "f8ce9d12-9c54-4ad8-cc24-20e18251d0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ submission.csv created with R² optimized blend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "X = train.drop(columns=[\"target\"])\n",
        "y = train[\"target\"]\n",
        "X_test = test.drop(columns=[\"id\"])\n",
        "\n",
        "# Feature engineering\n",
        "for df in [X, X_test]:\n",
        "    df[\"row_mean\"] = df.mean(axis=1)\n",
        "    df[\"row_std\"] = df.std(axis=1)\n",
        "    df[\"row_max\"] = df.max(axis=1)\n",
        "    df[\"row_min\"] = df.min(axis=1)\n",
        "    df[\"row_range\"] = df[\"row_max\"] - df[\"row_min\"]\n",
        "\n",
        "# K-Fold stacking setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros((X.shape[0], 3))\n",
        "test_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    lgb = LGBMRegressor(num_leaves=20, max_depth=10, learning_rate=0.05,\n",
        "                        n_estimators=200, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
        "    cat = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6,\n",
        "                            loss_function='RMSE', early_stopping_rounds=50, verbose=0, random_state=42)\n",
        "    rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "    lgb.fit(X_tr, y_tr)\n",
        "    cat.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
        "    rf.fit(X_tr, y_tr)\n",
        "\n",
        "    oof_preds[val_idx, 0] = lgb.predict(X_val)\n",
        "    oof_preds[val_idx, 1] = cat.predict(X_val)\n",
        "    oof_preds[val_idx, 2] = rf.predict(X_val)\n",
        "\n",
        "    test_preds[:, 0] += lgb.predict(X_test) / 5\n",
        "    test_preds[:, 1] += cat.predict(X_test) / 5\n",
        "    test_preds[:, 2] += rf.predict(X_test) / 5\n",
        "\n",
        "# Train XGBoost meta-model\n",
        "meta = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "meta.fit(oof_preds, y)\n",
        "final_test_pred = meta.predict(test_preds)\n",
        "\n",
        "# Evaluate OOF R2\n",
        "print(\"Final OOF R2:\", r2_score(y, meta.predict(oof_preds)))\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_test_pred\n",
        "})\n",
        "submission.to_csv(\"submission_kfold_stack.csv\", index=False)\n",
        "print(\"✅ submission_kfold_stack.csv saved!\")\n"
      ],
      "metadata": {
        "id": "pMNl5-1MomFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}