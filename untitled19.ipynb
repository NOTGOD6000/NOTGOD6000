{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtUPm6ibyH4Qr/v1+mIbk0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NOTGOD6000/NOTGOD6000/blob/main/untitled19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "train=pd.read_csv(\"train.csv\")\n",
        "test=pd.read_csv(\"test.csv\")\n",
        "# Load data\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature Engineering\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "# Train-validation split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBM datasets\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
        "\n",
        "# Parameters\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Training\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    valid_sets=[train_data, valid_data],\n",
        "    valid_names=['train', 'valid'],\n",
        "    num_boost_round=1000,\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=50)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Prediction and Evaluation\n",
        "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "print(\"LightGBM Validation R2 with feature engineering:\", r2_score(y_valid, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m29pAPURg4E",
        "outputId": "123145a5-2790-4f49-83f2-c3c7fe4ccf97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttrain's rmse: 4.64226\tvalid's rmse: 5.15941\n",
            "[100]\ttrain's rmse: 3.93122\tvalid's rmse: 4.96798\n",
            "[150]\ttrain's rmse: 3.43076\tvalid's rmse: 4.90059\n",
            "[200]\ttrain's rmse: 3.0314\tvalid's rmse: 4.8734\n",
            "[250]\ttrain's rmse: 2.69365\tvalid's rmse: 4.84962\n",
            "Early stopping, best iteration is:\n",
            "[239]\ttrain's rmse: 2.76226\tvalid's rmse: 4.84616\n",
            "LightGBM Validation R2 with feature engineering: 0.44016539694931933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load your train.csv\n",
        "train = pd.read_csv('train.csv')\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature Engineering\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "# Log-transform the target\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# Split\n",
        "X_train, X_valid, y_train_log, y_valid = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBM datasets\n",
        "train_data = lgb.Dataset(X_train, label=y_train_log)\n",
        "valid_data = lgb.Dataset(X_valid, label=np.log1p(y_valid))\n",
        "\n",
        "# Parameters\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Train\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    valid_sets=[train_data, valid_data],\n",
        "    valid_names=['train', 'valid'],\n",
        "    num_boost_round=1000,\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=50)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Predict and inverse log\n",
        "y_pred_log = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "\n",
        "# R2 Score\n",
        "print(\"Log-transformed LightGBM R2:\", r2_score(np.expm1(y_valid), y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTB9IRyrV0wV",
        "outputId": "0233fd12-401f-4be8-ef1e-bd1d748d4b49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttrain's rmse: 0.0307086\tvalid's rmse: 3.22594\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's rmse: 0.0438801\tvalid's rmse: 3.22535\n",
            "Log-transformed LightGBM R2: 0.020977588270903236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RdFokaaQPTl",
        "outputId": "14514b4e-4133-4890-a9ce-68a8b3d65745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
            "Fast Tuned LightGBM R2: 0.4193542365673183\n",
            "Best Params: {'subsample': 0.8, 'num_leaves': 20, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature engineering (same as before)\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "# Train/validation split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fast param grid\n",
        "param_dist = {\n",
        "    'num_leaves': [20, 40],\n",
        "    'max_depth': [5, 10],\n",
        "    'learning_rate': [0.03, 0.05],\n",
        "    'n_estimators': [100, 200],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [0.8]\n",
        "}\n",
        "\n",
        "# LightGBM Regressor\n",
        "lgb_model = LGBMRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Faster RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=lgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=6,           # small number of combinations\n",
        "    cv=2,               # faster cross-validation\n",
        "    scoring='r2',\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_valid)\n",
        "\n",
        "print(\"Fast Tuned LightGBM R2:\", r2_score(y_valid, y_pred))\n",
        "print(\"Best Params:\", random_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxApT6OaVHpB",
        "outputId": "38c84f5f-ce3d-41dc-d502-97e836af4bda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature engineering\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. LightGBM with best params\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    subsample=0.8,\n",
        "    num_leaves=20,\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.05,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_pred = lgb_model.predict(X_valid)\n",
        "\n",
        "# 2. CatBoost\n",
        "cat_model = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    loss_function='RMSE',\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
        "cat_pred = cat_model.predict(X_valid)\n",
        "\n",
        "# 3. Random Forest\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_valid)\n",
        "\n",
        "# Blend predictions (simple average)\n",
        "final_pred = (lgb_pred + cat_pred + rf_pred) / 3\n",
        "\n",
        "# Evaluate\n",
        "print(\"Blended R2 score:\", r2_score(y_valid, final_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTpEV61-UOAb",
        "outputId": "f292031c-1a6d-40da-e963-bbc3ea2d5c3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended R2 score: 0.4322006457962714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A: LGBM heavy\n",
        "final_pred_A = 0.7 * lgb_pred + 0.2 * cat_pred + 0.1 * rf_pred\n",
        "print(\"A: 70% LGBM, 20% Cat, 10% RF R2:\", r2_score(y_valid, final_pred_A))\n",
        "\n",
        "# B: LGBM + Cat only\n",
        "final_pred_B = 0.6 * lgb_pred + 0.4 * cat_pred\n",
        "print(\"B: 60% LGBM, 40% Cat R2:\", r2_score(y_valid, final_pred_B))\n",
        "\n",
        "# C: Equal strong models\n",
        "final_pred_C = 0.5 * lgb_pred + 0.5 * cat_pred\n",
        "print(\"C: 50% LGBM, 50% Cat R2:\", r2_score(y_valid, final_pred_C))\n",
        "\n",
        "# D: CatBoost heavy\n",
        "final_pred_D = 0.4 * lgb_pred + 0.5 * cat_pred + 0.1 * rf_pred\n",
        "print(\"D: 40% LGBM, 50% Cat, 10% RF R2:\", r2_score(y_valid, final_pred_D))\n",
        "\n",
        "# E: LGBM only (best single model)\n",
        "print(\"E: 100% LGBM R2:\", r2_score(y_valid, lgb_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZGRNboLWtru",
        "outputId": "98bd5af2-8dce-4047-d7f2-1d44c675d108"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: 70% LGBM, 20% Cat, 10% RF R2: 0.4318064648933103\n",
            "B: 60% LGBM, 40% Cat R2: 0.44334896689371717\n",
            "C: 50% LGBM, 50% Cat R2: 0.447471750530551\n",
            "D: 40% LGBM, 50% Cat, 10% RF R2: 0.44632283899384273\n",
            "E: 100% LGBM R2: 0.4193542365673183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test set\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "X_test = test.drop(columns=['id']).copy()\n",
        "\n",
        "# Apply same feature engineering\n",
        "X_test['row_mean'] = X_test.mean(axis=1)\n",
        "X_test['row_std'] = X_test.std(axis=1)\n",
        "X_test['row_max'] = X_test.max(axis=1)\n",
        "X_test['row_min'] = X_test.min(axis=1)\n",
        "X_test['row_range'] = X_test['row_max'] - X_test['row_min']\n",
        "\n",
        "# Predict using LightGBM and CatBoost only\n",
        "lgb_test_pred = lgb_model.predict(X_test)\n",
        "cat_test_pred = cat_model.predict(X_test)\n",
        "\n",
        "# Best blend\n",
        "final_test_pred = 0.5 * lgb_test_pred + 0.5 * cat_test_pred\n",
        "\n",
        "# Save to CSV\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'target': final_test_pred\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"✅ submission.csv created with R² optimized blend.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOnoxgVmXZcY",
        "outputId": "6d665100-24f1-4aaa-f1aa-c42216c20db0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ submission.csv created with R² optimized blend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Base features\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "# New feature interactions\n",
        "X['f1_f2'] = X['f1'] * X['f2']\n",
        "X['f3_div_f4'] = X['f3'] / (X['f4'] + 1e-5)\n",
        "X['f5_plus_f6'] = X['f5'] + X['f6']\n",
        "X['f7_minus_f8'] = X['f7'] - X['f8']\n",
        "X['f9_times_f10'] = X['f9'] * X['f10']\n",
        "\n",
        "# Train/validation split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Best LGBM config\n",
        "model = lgb.LGBMRegressor(\n",
        "    subsample=0.8,\n",
        "    num_leaves=20,\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.05,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_valid)\n",
        "print(\"R2 with interaction features:\", r2_score(y_valid, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYigv-s6cIff",
        "outputId": "2c7e4ca8-85a4-4700-aabf-3e9f7871aee1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 with interaction features: 0.4206081310820652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Load and split data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature engineering\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize base models\n",
        "lgb_model = LGBMRegressor(num_leaves=20, max_depth=10, learning_rate=0.05,\n",
        "                          n_estimators=200, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
        "\n",
        "cat_model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6,\n",
        "                              loss_function='RMSE', early_stopping_rounds=50, verbose=0, random_state=42)\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train base models on training set\n",
        "lgb_model.fit(X_train, y_train)\n",
        "cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get base model predictions for validation set\n",
        "lgb_val_pred = lgb_model.predict(X_valid)\n",
        "cat_val_pred = cat_model.predict(X_valid)\n",
        "rf_val_pred = rf_model.predict(X_valid)\n",
        "\n",
        "# Stack base predictions into new feature matrix\n",
        "stacked_val = np.column_stack([lgb_val_pred, cat_val_pred, rf_val_pred])\n",
        "\n",
        "# Train meta-model (Ridge regression) on stacked predictions\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "meta_model.fit(stacked_val, y_valid)\n",
        "\n",
        "# Final stacked prediction\n",
        "meta_pred = meta_model.predict(stacked_val)\n",
        "print(\"Stacked Ridge Meta-Model R2:\", r2_score(y_valid, meta_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIZJrTGLcbZg",
        "outputId": "01e322d1-2a91-48db-ed4d-4b8ae055fe6a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacked Ridge Meta-Model R2: 0.4597593502268543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test set\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "X_test = test.drop(columns=['id']).copy()\n",
        "\n",
        "# Same feature engineering\n",
        "X_test['row_mean'] = X_test.mean(axis=1)\n",
        "X_test['row_std'] = X_test.std(axis=1)\n",
        "X_test['row_max'] = X_test.max(axis=1)\n",
        "X_test['row_min'] = X_test.min(axis=1)\n",
        "X_test['row_range'] = X_test['row_max'] - X_test['row_min']\n",
        "\n",
        "# Base model predictions on test set\n",
        "lgb_test_pred = lgb_model.predict(X_test)\n",
        "cat_test_pred = cat_model.predict(X_test)\n",
        "rf_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Stack predictions for meta-model\n",
        "stacked_test = np.column_stack([lgb_test_pred, cat_test_pred, rf_test_pred])\n",
        "final_test_pred = meta_model.predict(stacked_test)\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'target': final_test_pred\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"✅ submission.csv created using stacked model (R² = 0.4598)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXksdSAqd4F4",
        "outputId": "cefb52b5-6196-46cc-b592-eb553d6b15bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ submission.csv created using stacked model (R² = 0.4598)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4nD0QVWeQLG",
        "outputId": "55e74992-5d0d-45b7-bf37-6f3f88b6dd17"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load and prepare data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "X = train.drop(columns=['target'])\n",
        "y = train['target']\n",
        "\n",
        "# Feature engineering\n",
        "X['row_mean'] = X.mean(axis=1)\n",
        "X['row_std'] = X.std(axis=1)\n",
        "X['row_max'] = X.max(axis=1)\n",
        "X['row_min'] = X.min(axis=1)\n",
        "X['row_range'] = X['row_max'] - X['row_min']\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Base models\n",
        "lgb_model = LGBMRegressor(num_leaves=20, max_depth=10, learning_rate=0.05,\n",
        "                          n_estimators=200, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
        "\n",
        "cat_model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6,\n",
        "                              loss_function='RMSE', early_stopping_rounds=50, verbose=0, random_state=42)\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit base models\n",
        "lgb_model.fit(X_train, y_train)\n",
        "cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get base predictions\n",
        "lgb_val_pred = lgb_model.predict(X_valid)\n",
        "cat_val_pred = cat_model.predict(X_valid)\n",
        "rf_val_pred = rf_model.predict(X_valid)\n",
        "\n",
        "# Stack base predictions\n",
        "stacked_val = np.column_stack([lgb_val_pred, cat_val_pred, rf_val_pred])\n",
        "\n",
        "# Train XGBoost as meta-model\n",
        "xgb_meta = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "xgb_meta.fit(stacked_val, y_valid)\n",
        "\n",
        "# Final prediction\n",
        "xgb_stack_pred = xgb_meta.predict(stacked_val)\n",
        "print(\"XGBoost Meta-Model R2:\", r2_score(y_valid, xgb_stack_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K74zyMVer5-",
        "outputId": "170c986e-ee8d-490c-b986-9ee7b41a9e03"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Meta-Model R2: 0.5914836756491184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "X_test = test.drop(columns=['id']).copy()\n",
        "\n",
        "# Apply same feature engineering\n",
        "X_test['row_mean'] = X_test.mean(axis=1)\n",
        "X_test['row_std'] = X_test.std(axis=1)\n",
        "X_test['row_max'] = X_test.max(axis=1)\n",
        "X_test['row_min'] = X_test.min(axis=1)\n",
        "X_test['row_range'] = X_test['row_max'] - X_test['row_min']\n",
        "\n",
        "# Base model predictions on test set\n",
        "lgb_test_pred = lgb_model.predict(X_test)\n",
        "cat_test_pred = cat_model.predict(X_test)\n",
        "rf_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Stack predictions for XGB meta-model\n",
        "stacked_test = np.column_stack([lgb_test_pred, cat_test_pred, rf_test_pred])\n",
        "final_test_pred = xgb_meta.predict(stacked_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'target': final_test_pred\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"✅ submission.csv created with XGBoost stacked model (R² = 0.5915)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlKL9HMqfRam",
        "outputId": "7f87e3be-e6a5-4fd1-cf7e-19bd729f3efa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ submission.csv created with XGBoost stacked model (R² = 0.5915)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "X = train.drop(columns=[\"target\"])\n",
        "y = train[\"target\"]\n",
        "X_test = test.drop(columns=[\"id\"])\n",
        "\n",
        "# Feature engineering\n",
        "for df in [X, X_test]:\n",
        "    df[\"row_mean\"] = df.mean(axis=1)\n",
        "    df[\"row_std\"] = df.std(axis=1)\n",
        "    df[\"row_max\"] = df.max(axis=1)\n",
        "    df[\"row_min\"] = df.min(axis=1)\n",
        "    df[\"row_range\"] = df[\"row_max\"] - df[\"row_min\"]\n",
        "\n",
        "# K-Fold stacking setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros((X.shape[0], 3))\n",
        "test_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    lgb = LGBMRegressor(num_leaves=20, max_depth=10, learning_rate=0.05,\n",
        "                        n_estimators=200, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
        "    cat = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6,\n",
        "                            loss_function='RMSE', early_stopping_rounds=50, verbose=0, random_state=42)\n",
        "    rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "    lgb.fit(X_tr, y_tr)\n",
        "    cat.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
        "    rf.fit(X_tr, y_tr)\n",
        "\n",
        "    oof_preds[val_idx, 0] = lgb.predict(X_val)\n",
        "    oof_preds[val_idx, 1] = cat.predict(X_val)\n",
        "    oof_preds[val_idx, 2] = rf.predict(X_val)\n",
        "\n",
        "    test_preds[:, 0] += lgb.predict(X_test) / 5\n",
        "    test_preds[:, 1] += cat.predict(X_test) / 5\n",
        "    test_preds[:, 2] += rf.predict(X_test) / 5\n",
        "\n",
        "# Train XGBoost meta-model\n",
        "meta = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "meta.fit(oof_preds, y)\n",
        "final_test_pred = meta.predict(test_preds)\n",
        "\n",
        "# Evaluate OOF R2\n",
        "print(\"Final OOF R2:\", r2_score(y, meta.predict(oof_preds)))\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_test_pred\n",
        "})\n",
        "submission.to_csv(\"submission_kfold_stack.csv\", index=False)\n",
        "print(\"✅ submission_kfold_stack.csv saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JHigwRZj8gY",
        "outputId": "2f428e51-3cf7-4d37-e5e6-57a97af71995"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final OOF R2: 0.47960563001360235\n",
            "✅ submission_kfold_stack.csv saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "X = train.drop(columns=[\"target\"])\n",
        "y = train[\"target\"]\n",
        "X_test = test.drop(columns=[\"id\"])\n",
        "\n",
        "# Feature engineering\n",
        "for df in [X, X_test]:\n",
        "    df[\"row_mean\"] = df.mean(axis=1)\n",
        "    df[\"row_std\"] = df.std(axis=1)\n",
        "    df[\"row_max\"] = df.max(axis=1)\n",
        "    df[\"row_min\"] = df.min(axis=1)\n",
        "    df[\"row_range\"] = df[\"row_max\"] - df[\"row_min\"]\n",
        "\n",
        "# Log-transform the target\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# K-Fold setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros((X.shape[0], 3))\n",
        "test_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]\n",
        "\n",
        "    lgb = LGBMRegressor(num_leaves=20, max_depth=10, learning_rate=0.05,\n",
        "                        n_estimators=200, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
        "    cat = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6,\n",
        "                            loss_function='RMSE', early_stopping_rounds=50, verbose=0, random_state=42)\n",
        "    rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "    lgb.fit(X_tr, y_tr)\n",
        "    cat.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
        "    rf.fit(X_tr, y_tr)\n",
        "\n",
        "    oof_preds[val_idx, 0] = lgb.predict(X_val)\n",
        "    oof_preds[val_idx, 1] = cat.predict(X_val)\n",
        "    oof_preds[val_idx, 2] = rf.predict(X_val)\n",
        "\n",
        "    test_preds[:, 0] += lgb.predict(X_test) / 5\n",
        "    test_preds[:, 1] += cat.predict(X_test) / 5\n",
        "    test_preds[:, 2] += rf.predict(X_test) / 5\n",
        "\n",
        "# Meta-model on log-space\n",
        "meta = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "meta.fit(oof_preds, y_log)\n",
        "\n",
        "# Predict and reverse transform\n",
        "stacked_log_pred = meta.predict(test_preds)\n",
        "stacked_pred = np.expm1(stacked_log_pred)\n",
        "\n",
        "# Blend with base model (e.g., LightGBM)\n",
        "final_pred = 0.6 * stacked_pred + 0.4 * np.expm1(test_preds[:, 0])\n",
        "\n",
        "# Save to submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_pred\n",
        "})\n",
        "submission.to_csv(\"submission_log_blend.csv\", index=False)\n",
        "\n",
        "# Evaluate local log-space R²\n",
        "print(\"OOF R² in log-space:\", r2_score(y_log, meta.predict(oof_preds)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFMtXrBCk61o",
        "outputId": "bdd6cfe9-3cf9-47e8-8ff8-f44ed22302e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF R² in log-space: 0.48435472014489167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "X = train.drop(columns=[\"target\"])\n",
        "y = train[\"target\"]\n",
        "X_test = test.drop(columns=[\"id\"])\n",
        "\n",
        "# Feature engineering\n",
        "for df in [X, X_test]:\n",
        "    df[\"row_mean\"] = df.mean(axis=1)\n",
        "    df[\"row_std\"] = df.std(axis=1)\n",
        "    df[\"row_max\"] = df.max(axis=1)\n",
        "    df[\"row_min\"] = df.min(axis=1)\n",
        "    df[\"row_range\"] = df[\"row_max\"] - df[\"row_min\"]\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=20)  # or use X.shape[1] - 1 for dynamic sizing\n",
        "X_pca = pca.fit_transform(X)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Prepare OOF and test containers\n",
        "oof_preds = np.zeros((X.shape[0], 5))\n",
        "test_preds = np.zeros((X_test.shape[0], 5))\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_pca)):\n",
        "    X_tr, X_val = X_pca[train_idx], X_pca[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Models\n",
        "    lgb = LGBMRegressor(num_leaves=20, max_depth=10, learning_rate=0.05, n_estimators=200)\n",
        "    cat = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, verbose=0, early_stopping_rounds=50)\n",
        "    rf = RandomForestRegressor(n_estimators=200, max_depth=10, n_jobs=-1)\n",
        "    hgb = HistGradientBoostingRegressor(max_iter=200, learning_rate=0.05, max_depth=10)\n",
        "    xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=5)\n",
        "\n",
        "    # Train\n",
        "    lgb.fit(X_tr, y_tr)\n",
        "    cat.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
        "    rf.fit(X_tr, y_tr)\n",
        "    hgb.fit(X_tr, y_tr)\n",
        "    xgb.fit(X_tr, y_tr)\n",
        "\n",
        "    # OOF\n",
        "    oof_preds[val_idx, 0] = lgb.predict(X_val)\n",
        "    oof_preds[val_idx, 1] = cat.predict(X_val)\n",
        "    oof_preds[val_idx, 2] = rf.predict(X_val)\n",
        "    oof_preds[val_idx, 3] = hgb.predict(X_val)\n",
        "    oof_preds[val_idx, 4] = xgb.predict(X_val)\n",
        "\n",
        "    # Test preds (average)\n",
        "    test_preds[:, 0] += lgb.predict(X_test_pca) / 5\n",
        "    test_preds[:, 1] += cat.predict(X_test_pca) / 5\n",
        "    test_preds[:, 2] += rf.predict(X_test_pca) / 5\n",
        "    test_preds[:, 3] += hgb.predict(X_test_pca) / 5\n",
        "    test_preds[:, 4] += xgb.predict(X_test_pca) / 5\n",
        "\n",
        "# Meta-model\n",
        "meta = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3)\n",
        "meta.fit(oof_preds, y)\n",
        "final_test_pred = meta.predict(test_preds)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Final OOF R²:\", r2_score(y, meta.predict(oof_preds)))\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_test_pred\n",
        "})\n",
        "submission.to_csv(\"submission_advanced_stack.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hue4t8emyus",
        "outputId": "397ac920-a0e8-4a23-872b-0a38b36f388b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final OOF R²: 0.4255236342502451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjMemnIJo1kP",
        "outputId": "e9f5f37f-4662-41c1-cf84-36783a9dff25"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.2 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "\n",
        "# Objective function\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    model = XGBRegressor(**params)\n",
        "    scores = cross_val_score(model, oof_preds, y, cv=5, scoring=\"r2\", n_jobs=-1)\n",
        "    return scores.mean()\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "# Best meta-model\n",
        "best_params = study.best_params\n",
        "best_params[\"random_state\"] = 42\n",
        "meta_model = XGBRegressor(**best_params)\n",
        "meta_model.fit(oof_preds, y)\n",
        "\n",
        "# Predict on test stacked predictions\n",
        "final_pred = meta_model.predict(test_preds)\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_pred\n",
        "})\n",
        "submission.to_csv(\"submission_optuna_xgb_meta.csv\", index=False)\n",
        "\n",
        "# Report\n",
        "print(\"Best R2:\", r2_score(y, meta_model.predict(oof_preds)))\n",
        "print(\"Best Params:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL4Q70hKo4mQ",
        "outputId": "ae1aa3d7-50d5-4eda-c5ef-f79f676f9912"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-21 17:57:56,799] A new study created in memory with name: no-name-03ec96c0-7f17-4445-860a-94ca5632ddee\n",
            "[I 2025-06-21 17:58:02,320] Trial 0 finished with value: 0.3547865191593881 and parameters: {'n_estimators': 199, 'max_depth': 4, 'learning_rate': 0.04752400619780795, 'subsample': 0.6436466628884236, 'colsample_bytree': 0.8899552397579427, 'gamma': 2.226781736794661, 'reg_alpha': 2.038016132256986, 'reg_lambda': 4.961239630298745}. Best is trial 0 with value: 0.3547865191593881.\n",
            "[I 2025-06-21 17:58:20,663] Trial 1 finished with value: 0.31481196604773876 and parameters: {'n_estimators': 441, 'max_depth': 10, 'learning_rate': 0.015777133737404893, 'subsample': 0.8106581079085065, 'colsample_bytree': 0.9846080829876719, 'gamma': 3.8494408466972088, 'reg_alpha': 0.05289859285011189, 'reg_lambda': 0.8489606980213493}. Best is trial 0 with value: 0.3547865191593881.\n",
            "[I 2025-06-21 17:58:21,181] Trial 2 finished with value: 0.298401209283078 and parameters: {'n_estimators': 211, 'max_depth': 4, 'learning_rate': 0.27094992999368067, 'subsample': 0.9715374594066837, 'colsample_bytree': 0.8269192690300815, 'gamma': 4.906539955895072, 'reg_alpha': 4.469253395196478, 'reg_lambda': 4.012938857353252}. Best is trial 0 with value: 0.3547865191593881.\n",
            "[I 2025-06-21 17:58:21,724] Trial 3 finished with value: 0.35247114689599734 and parameters: {'n_estimators': 275, 'max_depth': 3, 'learning_rate': 0.06710769306493201, 'subsample': 0.9201299763341486, 'colsample_bytree': 0.8286425145040996, 'gamma': 1.8581384652225152, 'reg_alpha': 1.047369045165345, 'reg_lambda': 1.3234200902339877}. Best is trial 0 with value: 0.3547865191593881.\n",
            "[I 2025-06-21 17:58:23,396] Trial 4 finished with value: 0.1768625539980209 and parameters: {'n_estimators': 359, 'max_depth': 10, 'learning_rate': 0.2274824301939676, 'subsample': 0.6118542811302266, 'colsample_bytree': 0.7966791281528822, 'gamma': 4.527128456193316, 'reg_alpha': 0.13120604545686587, 'reg_lambda': 1.6626160277737156}. Best is trial 0 with value: 0.3547865191593881.\n",
            "[I 2025-06-21 17:58:23,894] Trial 5 finished with value: 0.3623801152204294 and parameters: {'n_estimators': 249, 'max_depth': 3, 'learning_rate': 0.02817737363286834, 'subsample': 0.6989098632933279, 'colsample_bytree': 0.7581100335676719, 'gamma': 2.9172836674237073, 'reg_alpha': 4.540394603946809, 'reg_lambda': 4.656972759634452}. Best is trial 5 with value: 0.3623801152204294.\n",
            "[I 2025-06-21 17:58:24,748] Trial 6 finished with value: 0.3394966855153522 and parameters: {'n_estimators': 475, 'max_depth': 3, 'learning_rate': 0.0659814039910931, 'subsample': 0.7068855027208465, 'colsample_bytree': 0.634793353052428, 'gamma': 1.8129058705392191, 'reg_alpha': 3.816699832408614, 'reg_lambda': 3.7233858915231597}. Best is trial 5 with value: 0.3623801152204294.\n",
            "[I 2025-06-21 17:58:25,811] Trial 7 finished with value: 0.22450691151940155 and parameters: {'n_estimators': 489, 'max_depth': 4, 'learning_rate': 0.22084376294077568, 'subsample': 0.9722672585237492, 'colsample_bytree': 0.6515482713010727, 'gamma': 0.31454519418398885, 'reg_alpha': 0.2671912015077149, 'reg_lambda': 0.38737742848166556}. Best is trial 5 with value: 0.3623801152204294.\n",
            "[I 2025-06-21 17:58:26,701] Trial 8 finished with value: 0.249090894024495 and parameters: {'n_estimators': 335, 'max_depth': 4, 'learning_rate': 0.2709693838770277, 'subsample': 0.8278551604595032, 'colsample_bytree': 0.8752972321541443, 'gamma': 2.1042300922833586, 'reg_alpha': 0.6646689592374683, 'reg_lambda': 4.956517503703366}. Best is trial 5 with value: 0.3623801152204294.\n",
            "[I 2025-06-21 17:58:28,395] Trial 9 finished with value: 0.3247807346582198 and parameters: {'n_estimators': 398, 'max_depth': 6, 'learning_rate': 0.03845895501069529, 'subsample': 0.7524825101159902, 'colsample_bytree': 0.7857461554378538, 'gamma': 2.9520299030039814, 'reg_alpha': 4.341848005368533, 'reg_lambda': 1.3067126861278355}. Best is trial 5 with value: 0.3623801152204294.\n",
            "[I 2025-06-21 17:58:28,993] Trial 10 finished with value: 0.30841504610107046 and parameters: {'n_estimators': 101, 'max_depth': 6, 'learning_rate': 0.010159208494610661, 'subsample': 0.6915715476438025, 'colsample_bytree': 0.714038777906141, 'gamma': 0.704615153497222, 'reg_alpha': 3.3768552879788, 'reg_lambda': 2.709561302360348}. Best is trial 5 with value: 0.3623801152204294.\n",
            "[I 2025-06-21 17:58:29,338] Trial 11 finished with value: 0.36368503414124354 and parameters: {'n_estimators': 185, 'max_depth': 2, 'learning_rate': 0.029607030084274347, 'subsample': 0.6026424198213325, 'colsample_bytree': 0.9425446016643346, 'gamma': 3.047484517738747, 'reg_alpha': 2.0662243381914993, 'reg_lambda': 4.98442664612331}. Best is trial 11 with value: 0.36368503414124354.\n",
            "[I 2025-06-21 17:58:29,753] Trial 12 finished with value: 0.3637411802075485 and parameters: {'n_estimators': 168, 'max_depth': 2, 'learning_rate': 0.02577988021758678, 'subsample': 0.6033592153607796, 'colsample_bytree': 0.98718297384362, 'gamma': 3.3165922210002625, 'reg_alpha': 2.4509142109369373, 'reg_lambda': 4.043403307272757}. Best is trial 12 with value: 0.3637411802075485.\n",
            "[I 2025-06-21 17:58:30,194] Trial 13 finished with value: 0.3595375154970214 and parameters: {'n_estimators': 120, 'max_depth': 2, 'learning_rate': 0.022983390388299302, 'subsample': 0.6020414511353335, 'colsample_bytree': 0.9835788742261989, 'gamma': 3.620683262557358, 'reg_alpha': 2.1979524368777956, 'reg_lambda': 3.306767079960009}. Best is trial 12 with value: 0.3637411802075485.\n",
            "[I 2025-06-21 17:58:33,015] Trial 14 finished with value: 0.2870604870139288 and parameters: {'n_estimators': 177, 'max_depth': 8, 'learning_rate': 0.0953998591613964, 'subsample': 0.6554319547930039, 'colsample_bytree': 0.927118031430465, 'gamma': 3.897120096252923, 'reg_alpha': 2.7658948755917936, 'reg_lambda': 4.103566903388623}. Best is trial 12 with value: 0.3637411802075485.\n",
            "[I 2025-06-21 17:58:33,340] Trial 15 finished with value: 0.36061313440326465 and parameters: {'n_estimators': 157, 'max_depth': 2, 'learning_rate': 0.019840998613851014, 'subsample': 0.8718931745063944, 'colsample_bytree': 0.9364211755091487, 'gamma': 3.0171534818406682, 'reg_alpha': 1.5091097039914003, 'reg_lambda': 2.894105044619721}. Best is trial 12 with value: 0.3637411802075485.\n",
            "[I 2025-06-21 17:58:35,399] Trial 16 finished with value: 0.3528766214122284 and parameters: {'n_estimators': 256, 'max_depth': 7, 'learning_rate': 0.01277581565650463, 'subsample': 0.7520498211283285, 'colsample_bytree': 0.9957266289757531, 'gamma': 3.4410140371241313, 'reg_alpha': 3.035934720636615, 'reg_lambda': 4.389734891031542}. Best is trial 12 with value: 0.3637411802075485.\n",
            "[I 2025-06-21 17:58:35,715] Trial 17 finished with value: 0.36388562624192405 and parameters: {'n_estimators': 162, 'max_depth': 2, 'learning_rate': 0.03346348192058747, 'subsample': 0.7512917273699846, 'colsample_bytree': 0.932634569390918, 'gamma': 1.2002031316467865, 'reg_alpha': 1.633179472235233, 'reg_lambda': 3.3911345511867754}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:36,334] Trial 18 finished with value: 0.3187421050175878 and parameters: {'n_estimators': 147, 'max_depth': 5, 'learning_rate': 0.12301650171997755, 'subsample': 0.7599940618693997, 'colsample_bytree': 0.8923751513971487, 'gamma': 1.120871119715291, 'reg_alpha': 1.2642317164417403, 'reg_lambda': 2.0983597898100035}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:39,443] Trial 19 finished with value: 0.3149914740657134 and parameters: {'n_estimators': 319, 'max_depth': 8, 'learning_rate': 0.03715189064055341, 'subsample': 0.8659411709043427, 'colsample_bytree': 0.852726835160017, 'gamma': 1.4117090889608703, 'reg_alpha': 1.64531976441626, 'reg_lambda': 3.4605537484218627}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:39,852] Trial 20 finished with value: 0.3631611147626229 and parameters: {'n_estimators': 234, 'max_depth': 2, 'learning_rate': 0.018152696001604077, 'subsample': 0.6535799129410866, 'colsample_bytree': 0.9396002802270058, 'gamma': 0.09276883395642876, 'reg_alpha': 2.493105170307027, 'reg_lambda': 2.27673568415107}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:40,150] Trial 21 finished with value: 0.3633502777782385 and parameters: {'n_estimators': 140, 'max_depth': 2, 'learning_rate': 0.02919292851927321, 'subsample': 0.6025205363893225, 'colsample_bytree': 0.9400669996027718, 'gamma': 2.4708204187954848, 'reg_alpha': 1.9359005568364616, 'reg_lambda': 3.1081187137772206}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:40,652] Trial 22 finished with value: 0.36288392940522946 and parameters: {'n_estimators': 186, 'max_depth': 3, 'learning_rate': 0.02887804167572682, 'subsample': 0.6691136285653047, 'colsample_bytree': 0.9579262023423413, 'gamma': 3.2471663425950377, 'reg_alpha': 2.4527688280962834, 'reg_lambda': 4.320153400329852}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:41,022] Trial 23 finished with value: 0.36298249688493645 and parameters: {'n_estimators': 214, 'max_depth': 2, 'learning_rate': 0.048659906330373114, 'subsample': 0.7157894524667886, 'colsample_bytree': 0.9587161783528872, 'gamma': 2.619181009895735, 'reg_alpha': 3.2096114920666428, 'reg_lambda': 3.8845201325753065}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:42,147] Trial 24 finished with value: 0.35569634538669453 and parameters: {'n_estimators': 288, 'max_depth': 5, 'learning_rate': 0.022606145627514142, 'subsample': 0.7732339110518573, 'colsample_bytree': 0.9049874690000743, 'gamma': 4.299995569442749, 'reg_alpha': 0.8499158274483172, 'reg_lambda': 4.598878450104838}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:42,572] Trial 25 finished with value: 0.3624437662690322 and parameters: {'n_estimators': 168, 'max_depth': 3, 'learning_rate': 0.03923111878996922, 'subsample': 0.6304327006011788, 'colsample_bytree': 0.9144528292886002, 'gamma': 1.3304643181876756, 'reg_alpha': 1.8567040377788868, 'reg_lambda': 3.7322010600223363}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:43,236] Trial 26 finished with value: 0.3451078452941033 and parameters: {'n_estimators': 104, 'max_depth': 5, 'learning_rate': 0.015454727454375499, 'subsample': 0.7280609373166863, 'colsample_bytree': 0.864072149450879, 'gamma': 2.553907001442714, 'reg_alpha': 3.6647031244082395, 'reg_lambda': 3.4310115332410467}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:43,655] Trial 27 finished with value: 0.36296083446120236 and parameters: {'n_estimators': 129, 'max_depth': 2, 'learning_rate': 0.07123340483481727, 'subsample': 0.6714611469186769, 'colsample_bytree': 0.963846381756297, 'gamma': 0.9073642682216889, 'reg_alpha': 2.8982183087240587, 'reg_lambda': 4.465489967022023}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:44,508] Trial 28 finished with value: 0.36098009682926246 and parameters: {'n_estimators': 225, 'max_depth': 3, 'learning_rate': 0.033146631381576315, 'subsample': 0.6279992184642085, 'colsample_bytree': 0.995652203720488, 'gamma': 4.072932140858336, 'reg_alpha': 2.2219614389267255, 'reg_lambda': 2.589072582265014}. Best is trial 17 with value: 0.36388562624192405.\n",
            "[I 2025-06-21 17:58:45,467] Trial 29 finished with value: 0.3567006873316843 and parameters: {'n_estimators': 191, 'max_depth': 4, 'learning_rate': 0.0505327917456227, 'subsample': 0.7913786881549744, 'colsample_bytree': 0.8916542569074841, 'gamma': 2.147090399526364, 'reg_alpha': 1.3639901328230177, 'reg_lambda': 4.97283485262809}. Best is trial 17 with value: 0.36388562624192405.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best R2: 0.3858450235795122\n",
            "Best Params: {'n_estimators': 162, 'max_depth': 2, 'learning_rate': 0.03346348192058747, 'subsample': 0.7512917273699846, 'colsample_bytree': 0.932634569390918, 'gamma': 1.2002031316467865, 'reg_alpha': 1.633179472235233, 'reg_lambda': 3.3911345511867754, 'random_state': 42}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "X = train.drop(columns=[\"target\"])\n",
        "y = train[\"target\"]\n",
        "X_test = test.drop(columns=[\"id\"])\n",
        "\n",
        "# Feature engineering\n",
        "for df in [X, X_test]:\n",
        "    df[\"row_mean\"] = df.mean(axis=1)\n",
        "    df[\"row_std\"] = df.std(axis=1)\n",
        "    df[\"row_max\"] = df.max(axis=1)\n",
        "    df[\"row_min\"] = df.min(axis=1)\n",
        "    df[\"row_range\"] = df[\"row_max\"] - df[\"row_min\"]\n",
        "\n",
        "# Setup for stacking\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros((X.shape[0], 3))       # LGBM, CatBoost, RF\n",
        "test_preds = np.zeros((X_test.shape[0], 3)) # same\n",
        "\n",
        "# K-Fold stacking\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Base models\n",
        "    lgb = LGBMRegressor(num_leaves=20, max_depth=10, learning_rate=0.05, n_estimators=200, random_state=42)\n",
        "    cat = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6,\n",
        "                            loss_function='RMSE', early_stopping_rounds=50, verbose=0, random_state=42)\n",
        "    rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit\n",
        "    lgb.fit(X_tr, y_tr)\n",
        "    cat.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
        "    rf.fit(X_tr, y_tr)\n",
        "\n",
        "    # OOF preds\n",
        "    oof_preds[val_idx, 0] = lgb.predict(X_val)\n",
        "    oof_preds[val_idx, 1] = cat.predict(X_val)\n",
        "    oof_preds[val_idx, 2] = rf.predict(X_val)\n",
        "\n",
        "    # Test preds\n",
        "    test_preds[:, 0] += lgb.predict(X_test) / n_splits\n",
        "    test_preds[:, 1] += cat.predict(X_test) / n_splits\n",
        "    test_preds[:, 2] += rf.predict(X_test) / n_splits\n",
        "\n",
        "# Meta-model\n",
        "meta = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "meta.fit(oof_preds, y)\n",
        "\n",
        "# Final prediction from stacked meta-model\n",
        "stacked_pred = meta.predict(test_preds)\n",
        "\n",
        "# Optional blend with LGBM base (adjust weights if needed)\n",
        "lgb_pred = test_preds[:, 0]\n",
        "final_pred = 0.6 * stacked_pred + 0.4 * lgb_pred\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_pred\n",
        "})\n",
        "submission.to_csv(\"submission_final_stack.csv\", index=False)\n",
        "\n",
        "# Local evaluation\n",
        "print(\"Final OOF R²:\", r2_score(y, meta.predict(oof_preds)))\n",
        "\n"
      ],
      "metadata": {
        "id": "SLkh8tvQrOFx",
        "outputId": "d7e616bd-26f0-4eba-cd52-b8f8fefae3fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final OOF R²: 0.47734974239954475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "selector = SelectKBest(score_func=f_regression, k=25)\n",
        "X_new = selector.fit_transform(X, y)\n"
      ],
      "metadata": {
        "id": "_pGFPJ9Erzbw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "X = train.drop(columns=[\"target\"])\n",
        "y = train[\"target\"]\n",
        "X_test = test.drop(columns=[\"id\"])\n",
        "\n",
        "# Feature engineering\n",
        "for df in [X, X_test]:\n",
        "    df[\"row_mean\"] = df.mean(axis=1)\n",
        "    df[\"row_std\"] = df.std(axis=1)\n",
        "    df[\"row_max\"] = df.max(axis=1)\n",
        "    df[\"row_min\"] = df.min(axis=1)\n",
        "    df[\"row_range\"] = df[\"row_max\"] - df[\"row_min\"]\n",
        "\n",
        "# Feature selection (top 25)\n",
        "selector = SelectKBest(score_func=f_regression, k=25)\n",
        "X_sel = selector.fit_transform(X, y)\n",
        "X_test_sel = selector.transform(X_test)\n",
        "\n",
        "# Stacking\n",
        "oof_preds = np.zeros((X.shape[0], 3))\n",
        "test_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_sel)):\n",
        "    X_tr, X_val = X_sel[train_idx], X_sel[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    lgb = LGBMRegressor(num_leaves=20, max_depth=10, learning_rate=0.05, n_estimators=200)\n",
        "    cat = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, verbose=0, early_stopping_rounds=50)\n",
        "    rf = RandomForestRegressor(n_estimators=200, max_depth=10)\n",
        "\n",
        "    lgb.fit(X_tr, y_tr)\n",
        "    cat.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
        "    rf.fit(X_tr, y_tr)\n",
        "\n",
        "    oof_preds[val_idx, 0] = lgb.predict(X_val)\n",
        "    oof_preds[val_idx, 1] = cat.predict(X_val)\n",
        "    oof_preds[val_idx, 2] = rf.predict(X_val)\n",
        "\n",
        "    test_preds[:, 0] += lgb.predict(X_test_sel) / 5\n",
        "    test_preds[:, 1] += cat.predict(X_test_sel) / 5\n",
        "    test_preds[:, 2] += rf.predict(X_test_sel) / 5\n",
        "\n",
        "# Meta-model\n",
        "meta = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3)\n",
        "meta.fit(oof_preds, y)\n",
        "stacked_pred = meta.predict(test_preds)\n",
        "\n",
        "# Optional blend\n",
        "final_pred = 0.6 * stacked_pred + 0.4 * test_preds[:, 0]\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_pred\n",
        "})\n",
        "submission.to_csv(\"submission_selected_stack.csv\", index=False)\n",
        "\n",
        "# Local R2\n",
        "print(\"OOF R²:\", r2_score(y, meta.predict(oof_preds)))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ea-B3mY0rzUA",
        "outputId": "cc7579cd-69e0-40cd-e34c-9dc85bc5e01d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF R²: 0.47328428216636065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Train meta-models on OOF predictions\n",
        "ridge = Ridge(alpha=1.0)\n",
        "xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "lgbm = LGBMRegressor(num_leaves=20, max_depth=6, learning_rate=0.05, n_estimators=200, random_state=42)\n",
        "\n",
        "ridge.fit(oof_preds, y)\n",
        "xgb.fit(oof_preds, y)\n",
        "lgbm.fit(oof_preds, y)\n",
        "\n",
        "# OOF predictions from meta-models\n",
        "ridge_oof = ridge.predict(oof_preds)\n",
        "xgb_oof = xgb.predict(oof_preds)\n",
        "lgbm_oof = lgbm.predict(oof_preds)\n",
        "\n",
        "# R² scores\n",
        "print(\"Ridge OOF R²:\", r2_score(y, ridge_oof))\n",
        "print(\"XGB OOF R²:\", r2_score(y, xgb_oof))\n",
        "print(\"LGBM OOF R²:\", r2_score(y, lgbm_oof))\n",
        "\n",
        "# ✅ Ensemble OOF predictions (equal weight)\n",
        "ensemble_oof_eq = (ridge_oof + xgb_oof + lgbm_oof) / 3\n",
        "print(\"Equal Weight Ensemble OOF R²:\", r2_score(y, ensemble_oof_eq))\n",
        "\n",
        "# ✅ Weighted Ensemble (based on performance)\n",
        "ensemble_oof_weighted = 0.1 * ridge_oof + 0.4 * xgb_oof + 0.5 * lgbm_oof\n",
        "print(\"Weighted Ensemble OOF R²:\", r2_score(y, ensemble_oof_weighted))\n",
        "\n",
        "# ====== Final Test Predictions ======\n",
        "ridge_test = ridge.predict(test_preds)\n",
        "xgb_test = xgb.predict(test_preds)\n",
        "lgbm_test = lgbm.predict(test_preds)\n",
        "\n",
        "# Final weighted test prediction\n",
        "final_pred = 0.1 * ridge_test + 0.4 * xgb_test + 0.5 * lgbm_test\n",
        "\n",
        "# Optional blend with base LGBM\n",
        "final_pred = 0.6 * final_pred + 0.4 * test_preds[:, 0]  # blend with LGBM base\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_pred\n",
        "})\n",
        "submission.to_csv(\"submission_meta_ensemble_weighted.csv\", index=False)\n",
        "print(\"✅ Saved: submission_meta_ensemble_weighted.csv\")\n"
      ],
      "metadata": {
        "id": "S9fSX4nDtWYR",
        "outputId": "ef85d273-182a-4d7c-9277-f73897eb815d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge OOF R²: 0.43693417302493875\n",
            "XGB OOF R²: 0.47328428216636065\n",
            "LGBM OOF R²: 0.5133638761807828\n",
            "Equal Weight Ensemble OOF R²: 0.4793623615196567\n",
            "Weighted Ensemble OOF R²: 0.4932606935104151\n",
            "✅ Saved: submission_meta_ensemble_weighted.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit meta-models on ALL OOF + y\n",
        "ridge = Ridge(alpha=1.0)\n",
        "xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "lgbm = LGBMRegressor(num_leaves=20, max_depth=6, learning_rate=0.05, n_estimators=200, random_state=42)\n",
        "\n",
        "ridge.fit(oof_preds, y)\n",
        "xgb.fit(oof_preds, y)\n",
        "lgbm.fit(oof_preds, y)\n",
        "\n",
        "# Predict on test\n",
        "ridge_test = ridge.predict(test_preds)\n",
        "xgb_test = xgb.predict(test_preds)\n",
        "lgbm_test = lgbm.predict(test_preds)\n",
        "\n",
        "# Final blend\n",
        "final_pred = 0.1 * ridge_test + 0.4 * xgb_test + 0.5 * lgbm_test\n",
        "final_pred = 0.6 * final_pred + 0.4 * test_preds[:, 0]  # Optional blend with LGBM base\n",
        "\n",
        "# Save\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": final_pred\n",
        "})\n",
        "submission.to_csv(\"submission_refit_full_meta.csv\", index=False)\n",
        "print(\"✅ Saved: submission_refit_full_meta.csv\")\n"
      ],
      "metadata": {
        "id": "xF3s7Y-vuP6o",
        "outputId": "0de2867f-c0d8-4a81-f7b0-9e486d80e909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: submission_refit_full_meta.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "X = train.drop(columns=[\"target\"])\n",
        "y = train[\"target\"]\n",
        "X_test = test.drop(columns=[\"id\"])\n",
        "\n",
        "# Optional: feature engineering\n",
        "for df in [X, X_test]:\n",
        "    df[\"row_mean\"] = df.mean(axis=1)\n",
        "    df[\"row_std\"] = df.std(axis=1)\n",
        "    df[\"row_max\"] = df.max(axis=1)\n",
        "    df[\"row_min\"] = df.min(axis=1)\n",
        "    df[\"row_range\"] = df[\"row_max\"] - df[\"row_min\"]\n",
        "\n",
        "# Bin target\n",
        "n_bins = 10\n",
        "kbin = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
        "y_bins = kbin.fit_transform(y.values.reshape(-1, 1)).astype(int).ravel()\n",
        "\n",
        "# Split for validation\n",
        "X_train, X_valid, y_train, y_valid, y_bins_train, y_bins_valid = train_test_split(\n",
        "    X, y, y_bins, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train classifier\n",
        "clf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "clf.fit(X_train, y_bins_train)\n",
        "\n",
        "# Train one regressor per bin\n",
        "bin_regressors = {}\n",
        "for b in range(n_bins):\n",
        "    idx = y_bins_train == b\n",
        "    reg = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
        "    reg.fit(X_train[idx], y_train[idx])\n",
        "    bin_regressors[b] = reg\n",
        "\n",
        "# Predict on validation\n",
        "bin_preds = clf.predict(X_valid)\n",
        "final_preds = np.zeros_like(y_valid)\n",
        "\n",
        "for b in range(n_bins):\n",
        "    idx = bin_preds == b\n",
        "    if np.any(idx):\n",
        "        final_preds[idx] = bin_regressors[b].predict(X_valid[idx])\n",
        "\n",
        "# Evaluate\n",
        "print(\"Hybrid Model Validation R2:\", r2_score(y_valid, final_preds))\n",
        "\n",
        "# ==== Predict on test set ====\n",
        "bin_test_preds = clf.predict(X_test)\n",
        "test_final = np.zeros(X_test.shape[0])\n",
        "\n",
        "for b in range(n_bins):\n",
        "    idx = bin_test_preds == b\n",
        "    if np.any(idx):\n",
        "        test_final[idx] = bin_regressors[b].predict(X_test[idx])\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": test_final\n",
        "})\n",
        "submission.to_csv(\"submission_hybrid_quantile.csv\", index=False)\n",
        "print(\"✅ Saved: submission_hybrid_quantile.csv\")\n"
      ],
      "metadata": {
        "id": "m7HkBRCCu-GD",
        "outputId": "b5e69fe2-2c1e-4f02-ad44-4b7c4c26adc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid Model Validation R2: -0.034081607010225445\n",
            "✅ Saved: submission_hybrid_quantile.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities for all bins\n",
        "proba_valid = clf.predict_proba(X_valid)\n",
        "proba_test = clf.predict_proba(X_test)\n",
        "\n",
        "# Predict using all regressors, weighted by classifier probability\n",
        "final_preds = np.zeros(len(X_valid))\n",
        "test_final = np.zeros(len(X_test))\n",
        "\n",
        "for b in range(n_bins):\n",
        "    reg = bin_regressors.get(b)\n",
        "    if reg:\n",
        "        final_preds += proba_valid[:, b] * reg.predict(X_valid)\n",
        "        test_final += proba_test[:, b] * reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import r2_score\n",
        "print(\"Soft Hybrid Model Validation R2:\", r2_score(y_valid, final_preds))\n",
        "\n",
        "# Save improved submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"target\": test_final\n",
        "})\n",
        "submission.to_csv(\"submission_hybrid_soft.csv\", index=False)\n",
        "print(\"✅ Saved: submission_hybrid_soft.csv\")\n"
      ],
      "metadata": {
        "id": "9xV-Za-lvaDz",
        "outputId": "8b2b66a3-3e94-44c3-d506-9d32ab0c8fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soft Hybrid Model Validation R2: 0.3145977432955087\n",
            "✅ Saved: submission_hybrid_soft.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_stack = pd.read_csv(\"submission_meta_ensemble_weighted.csv\")\n",
        "sub_soft = pd.read_csv(\"submission_hybrid_soft.csv\")\n",
        "\n",
        "# 70% stack + 30% soft hybrid (or try 80/20 if hybrid is noisy)\n",
        "blended = sub_stack.copy()\n",
        "blended[\"target\"] = 0.7 * sub_stack[\"target\"] + 0.3 * sub_soft[\"target\"]\n",
        "\n",
        "blended.to_csv(\"submission_final_blend.csv\", index=False)\n",
        "print(\"✅ Saved: submission_final_blend.csv\")\n"
      ],
      "metadata": {
        "id": "M6lCVX8dvmX-",
        "outputId": "db173225-db96-45bf-a2cd-bfdd3efed4df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: submission_final_blend.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub1 = pd.read_csv(\"submission_kfold_stack.csv\")         # 0.5\n",
        "sub2 = pd.read_csv(\"submission_log_blend.csv\")           # 0.3\n",
        "sub3 = pd.read_csv(\"submission_meta_ensemble_weighted.csv\")  # 0.2\n",
        "\n",
        "blend = sub1.copy()\n",
        "blend[\"target\"] = (\n",
        "    0.5 * sub1[\"target\"] +\n",
        "    0.3 * sub2[\"target\"] +\n",
        "    0.2 * sub3[\"target\"]\n",
        ")\n",
        "\n",
        "blend.to_csv(\"submission_final_top3_blend.csv\", index=False)\n",
        "print(\"✅ Saved: submission_final_top3_blend.csv\")\n"
      ],
      "metadata": {
        "id": "82sZOOAawROn",
        "outputId": "a535d453-ef39-47f7-a54a-cee91762f9a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: submission_final_top3_blend.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "sub1 = pd.read_csv(\"submission_kfold_stack.csv\")\n",
        "sub2 = pd.read_csv(\"submission_log_blend.csv\")\n",
        "sub3 = pd.read_csv(\"submission_meta_ensemble_weighted.csv\")\n",
        "\n",
        "# Try weights like (0.5, 0.3, 0.2), etc.\n",
        "best_score = -np.inf\n",
        "best_weights = None\n",
        "best_submission = None\n",
        "\n",
        "# Simulate weights in 0.1 steps (they must sum to 1)\n",
        "for w1, w2, w3 in product(np.arange(0.1, 1.0, 0.1), repeat=3):\n",
        "    if abs((w1 + w2 + w3) - 1.0) > 1e-6:\n",
        "        continue\n",
        "    blend = w1 * sub1[\"target\"] + w2 * sub2[\"target\"] + w3 * sub3[\"target\"]\n",
        "\n",
        "    # For Kaggle: we can't evaluate without y_true, so skip scoring here.\n",
        "    # Instead, just save all blends and submit top few manually.\n",
        "    fname = f\"blend_{int(w1*100)}_{int(w2*100)}_{int(w3*100)}.csv\"\n",
        "    pd.DataFrame({\n",
        "        \"id\": sub1[\"id\"],\n",
        "        \"target\": blend\n",
        "    }).to_csv(fname, index=False)\n"
      ],
      "metadata": {
        "id": "o8CWZR1bxsM6"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}